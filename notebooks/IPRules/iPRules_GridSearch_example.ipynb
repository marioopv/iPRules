{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iPRules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "#Load Dataset\n",
    "dataset = load_iris()#pd.read_csv('./data/clean_dataset.csv')\n",
    "print(dataset.feature_names)\n",
    "#Define dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.1, random_state=1)\n",
    "\n",
    "\n",
    "# np.c_ is the numpy concatenate function\n",
    "# which is used to concat iris['data'] and iris['target'] arrays\n",
    "# for pandas column argument: concat iris['feature_names'] list\n",
    "# and string list (in this case one string); you can make this anything you'd like..\n",
    "# the original dataset would probably call this ['Species']\n",
    "pandas_dataset = pd.DataFrame(data= np.c_[dataset['data'], dataset['target']],\n",
    "                     columns= dataset['feature_names'] + ['target'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_score, make_scorer, recall_score, accuracy_score\n",
    "\n",
    "# Define scorer\n",
    "custom_scorer = make_scorer(accuracy_score, greater_is_better=True)\n",
    "param_grid = {\n",
    "        'n_estimators': [50, 100, 150, 200, 250, 300],  # being the number of trees in the forest.\n",
    "        'min_samples_leaf': [3], # number of minimum samples required at a leaf node.\n",
    "        'min_samples_split': [6], # number of minimum samples required to split an internal node.\n",
    "        'criterion': ['entropy','gini'], # measures the quality of a split. Can use gini's impurity or entropy.\n",
    "        }\n",
    "clf = GridSearchCV(\n",
    "        # Evaluates the performance of different groups of parameters for a model based on cross-validation.\n",
    "        RandomForestClassifier(),\n",
    "        param_grid,  # dict of parameters.\n",
    "        cv=10,  # Specified number of folds in the Cross-Validation(K-Fold).\n",
    "        scoring=custom_scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the random forest classifier on the Iris dataset\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions for the test set\n",
    "y_pred_test = clf.predict(X_test)\n",
    "\n",
    "# Get best estimator\n",
    "model = clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iPRules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 3 2]\n",
      "lista\n",
      "(array([0.11399136, 0.01193564, 0.45886089, 0.41521211]),)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 13\u001B[0m\n\u001B[1;32m      4\u001B[0m rules \u001B[38;5;241m=\u001B[39m iPRules(\n\u001B[1;32m      5\u001B[0m                 target_value_name \u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtarget\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      6\u001B[0m                 feature_coefficient\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.85\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      9\u001B[0m                 probability \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.95\u001B[39m\n\u001B[1;32m     10\u001B[0m             )\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# Fit model\u001B[39;00m\n\u001B[0;32m---> 13\u001B[0m \u001B[43mrules\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpandas_dataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28mprint\u001B[39m(rules\u001B[38;5;241m.\u001B[39mobtain_patterns())\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28mprint\u001B[39m(rules)\n",
      "File \u001B[0;32m~/Documents/GitHub/Projects/iPRules/iPRules/iPRules.py:271\u001B[0m, in \u001B[0;36miPRules.fit\u001B[0;34m(self, pandas_dataset)\u001B[0m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    266\u001B[0m \u001B[38;5;124;03mGet list of top features and generate rules\u001B[39;00m\n\u001B[1;32m    267\u001B[0m \u001B[38;5;124;03m:param pandas_dataset:\u001B[39;00m\n\u001B[1;32m    268\u001B[0m \u001B[38;5;124;03m:return:\u001B[39;00m\n\u001B[1;32m    269\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    270\u001B[0m \u001B[38;5;66;03m# List of top % important features in the model are obtained. This % regulated by coefficient between [0,1].\u001B[39;00m\n\u001B[0;32m--> 271\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmost_important_features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_top_important_features_list\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    272\u001B[0m \u001B[38;5;66;03m# Genera el árbol binario y obtiene las combinaciones que indican que hay un patrón:\u001B[39;00m\n\u001B[1;32m    273\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbinary_tree_generator(pandas_dataset)\n",
      "File \u001B[0;32m~/Documents/GitHub/Projects/iPRules/iPRules/iPRules.py:119\u001B[0m, in \u001B[0;36miPRules.get_top_important_features_list\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    117\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlista\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeature_importance_list)\n\u001B[0;32m--> 119\u001B[0m max_coefficient \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeature_importance_list\u001B[49m\u001B[43m[\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m  \u001B[38;5;66;03m# Valor de la característica más importante\u001B[39;00m\n\u001B[1;32m    120\u001B[0m \u001B[38;5;28mprint\u001B[39m(max_coefficient)\n\u001B[1;32m    121\u001B[0m features \u001B[38;5;241m=\u001B[39m []\n",
      "\u001B[0;31mTypeError\u001B[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "from iPRules.iPRules import iPRules\n",
    "\n",
    "# initialize\n",
    "rules = iPRules(\n",
    "                target_value_name =\"target\",\n",
    "                feature_coefficient=0.85,\n",
    "                feature_names=dataset.feature_names,\n",
    "                feature_importance_list=model.feature_importances_,\n",
    "                probability = 0.95\n",
    "            )\n",
    "\n",
    "# Fit model\n",
    "rules.fit(pandas_dataset)\n",
    "print(rules.obtain_patterns())\n",
    "print(rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
